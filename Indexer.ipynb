{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "from heapq import *\n",
    "from collections import *\n",
    "import xml.etree.cElementTree as et\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import pickle\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_path = \"article.xml\"\n",
    "print(wiki_path)\n",
    "index_path = \"Indexer\"\n",
    "print(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = {}  #stores all the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes={}\n",
    "indexes['t']=defaultdict(list) #titles\n",
    "indexes['c']=defaultdict(list) #category\n",
    "indexes['i']=defaultdict(list) #infobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = ['t','i','c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_count = 0\n",
    "pages_per_file = 50000\n",
    "#pages_per_file = 5000\n",
    "page_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files=list()    #stores pointers to output files\n",
    "title_pos=list()  #stores position of title words\n",
    "word_pos=dict()    #stores position of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE to remove urls\n",
    "re_url = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',re.DOTALL)\n",
    "\n",
    "# RE to remove tags & css\n",
    "re_tag = re.compile(r'{\\|(.*?)\\|}',re.DOTALL)\n",
    "\n",
    "# Regular Expression to remove {{cite **}} or {{vcite **}}\n",
    "re_cite = re.compile(r'{{v?cite(.*?)}}',re.DOTALL)\n",
    "\n",
    "# Regular Expression to remove [[file:]]\n",
    "re_file = re.compile(r'\\[\\[file:(.*?)\\]\\]',re.DOTALL)\n",
    "\n",
    "# pattern to get only alphnumeric text\n",
    "re_text = re.compile(\"[^a-zA-Z0-9]\")\n",
    "\n",
    "\n",
    "re_x={'c':\"\\[\\[Category:(.*?)\\]\\]\",'i':\"{{Infobox((.|\\n)*?)}}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stores all the stop words in a dictionary\n",
    "def stop_func():\n",
    "    reg = re.compile(\"\\\"|,| \")\n",
    "    file = open(\"stop_words.txt\",\"r\")\n",
    "    words = file.read()\n",
    "    words = re.split(reg,words)\n",
    "    for word in words:\n",
    "        if word:\n",
    "            stopword[word]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stems the given word and maintains the dictionary\n",
    "def preprocess_word(word):\n",
    "    global stem_word_dict\n",
    "    word = word.strip().lower()\n",
    "    if word not in stem_word_dict:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        stem_word_dict[word] = stem_word\n",
    "    else:\n",
    "        stem_word = stem_word_dict[word]\n",
    "    return stem_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes urls,tags,citations,files from given text\n",
    "def clean_text(text):\n",
    "    text = re_url.sub('',text)\n",
    "    text = re_tag.sub('',text)\n",
    "    text = re_cite.sub('',text)\n",
    "    text = re_file.sub('',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maintain_helper(text,x,word_map):\n",
    "    text = re.split(re_text, text)\n",
    "    for t in text:\n",
    "        t = preprocess_word(t)\n",
    "        if t and len(t) > 2 and t not in stopword:\n",
    "            if t not in word_map[x]:\n",
    "                word_map[x][t] = 1\n",
    "            else:\n",
    "                word_map[x][t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maintain(text,word_map,x):\n",
    "    try:\n",
    "        tempword = re.findall(re_x[x],text)\n",
    "        #if x=='i':\n",
    "         #   print(\"something\")\n",
    "        if tempword:\n",
    "            for temp in tempword:\n",
    "                if x=='c':\n",
    "                    maintain_helper(temp,x,word_map)\n",
    "                elif x=='i':\n",
    "                    for word in temp:\n",
    "                        maintain_helper(word,x,word_map)\n",
    "\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maintain2(text,word_map,x):\n",
    "    try:\n",
    "        #if x == 'b':\n",
    "            #text = re.split(re_text, text.lower())\n",
    "        if x == 't':\n",
    "            title_pos.append(title_tags.tell())\n",
    "            title_string = text\n",
    "            title_tags.write(title_string+\"\\n\")\n",
    "            text = re.split(re_text,text)\n",
    "        for word in text:\n",
    "            if word:\n",
    "                if word not in stem_word_dict:\n",
    "                    stem_word = stemmer.stem(word)\n",
    "                    stem_word_dict[word] = stem_word\n",
    "                else:\n",
    "                    stem_word = stem_word_dict[word]\n",
    "                word = stem_word\n",
    "                if word not in stopword and len(word)>2:\n",
    "                    if word not in word_map[x]:\n",
    "                        word_map[x][word] = 1\n",
    "                    else:\n",
    "                        word_map[x][word] += 1\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(word_map,id1):\n",
    "    for x in arr:\n",
    "        #print(x,\" \",len(word_map[x]))\n",
    "        for word in word_map[x]:\n",
    "            s = id1 + \":\"\n",
    "            s = s + str(word_map[x][word])\n",
    "            indexes[x][word].append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_files():\n",
    "    for x in arr:\n",
    "        print(\"writing...\")\n",
    "        file = index_path +\"/\"+ x +str(file_count) + \".txt\"\n",
    "        outfile = open(file,\"w+\")\n",
    "        for word in sorted(indexes[x]):\n",
    "            post_list = \",\".join(indexes[x][word])\n",
    "            index = word + \"-\" + post_list\n",
    "            outfile.write(index+\"\\n\")\n",
    "        outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlFile = wiki_path\n",
    "content = iter(et.iterparse(xmlFile,events=(\"start\",\"end\")))   #iterable content for xml file\n",
    "title_tags = open(index_path+\"/title_tags.txt\",\"w+\")\n",
    "stem_word_dict=dict()  #maps word to stem word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event,elem in content:\n",
    "    cnt+=1\n",
    "    tag = re.sub(r\"{.*}\", \"\", elem.tag)\n",
    "    if event == \"start\":\n",
    "        if tag == \"page\":\n",
    "            word_map={'t':{},'b':{},'i':{},'c':{}}\n",
    "            page_count = page_count + 1\n",
    "    if event == \"end\":\n",
    "        if tag == \"text\":\n",
    "            text = clean_text(str(elem.text))\n",
    "            maintain(text,word_map,'i')\n",
    "            #print(\"info finish\")\n",
    "            maintain(text,word_map,'c')\n",
    "            #maintain2(text,word_map,'b')\n",
    "\n",
    "        if tag == \"title\":\n",
    "            maintain2(str(elem.text),word_map,'t')\n",
    "\n",
    "        if tag == \"page\":\n",
    "            id1 = str(page_count)\n",
    "            to_index(word_map,id1)\n",
    "\n",
    "            if page_count % 50000 == 0:\n",
    "                stem_word_dict = {}\n",
    "\n",
    "            if page_count % pages_per_file == 0:\n",
    "                write_to_files()\n",
    "                for x in arr:\n",
    "                    indexes[x].clear()\n",
    "                file_count+=1\n",
    "        elem.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_files()\n",
    "file_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_file = index_path + \"/title_positions.pickle\"\n",
    "file = open(t_file,\"wb+\")\n",
    "pickle.dump(title_pos,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in arr:\n",
    "    print(\"sorting...\")\n",
    "    heap = []\n",
    "    flag = True\n",
    "    input_files = []\n",
    "    file = index_path + \"/\" + x + \".txt\"\n",
    "    fp = open(file,\"w+\")\n",
    "    output_files.append(fp)\n",
    "    outfile_index = len(output_files) - 1\n",
    "\n",
    "    for i in range(file_count):\n",
    "        file = index_path + \"/\" + x + str(i) + \".txt\"\n",
    "        if os.stat(file).st_size == 0:\n",
    "            try:\n",
    "                del input_files[i]\n",
    "                os.remove(file)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            fp = open(file,\"r\")\n",
    "            input_files.append(fp)\n",
    "\n",
    "    if len(input_files) == 0:\n",
    "        flag = False\n",
    "        break\n",
    "\n",
    "    for i in range(file_count):\n",
    "        try:\n",
    "            s = input_files[i].readline()[:-1]\n",
    "            heap.append((s,i))\n",
    "        except:\n",
    "            pass #flag=False\n",
    "\n",
    "    heapify(heap)\n",
    "    i=0\n",
    "\n",
    "    try:\n",
    "        while i < file_count:\n",
    "            s,ind = heappop(heap)\n",
    "            pos = s.find(\"-\")\n",
    "            word = s[:pos]\n",
    "            post_list = s[pos+1:]\n",
    "            next_line = input_files[ind].readline()[:-1]\n",
    "            if next_line:\n",
    "                heappush(heap,(next_line,ind))\n",
    "            else:\n",
    "                i+=1\n",
    "\n",
    "            if i == file_count:\n",
    "                flag=False\n",
    "                break\n",
    "\n",
    "            while i < file_count:\n",
    "                next_s , next_ind = heappop(heap)\n",
    "                next_pos = next_s.find(\"-\")\n",
    "                next_word = next_s[:next_pos]\n",
    "                next_post_list = next_s[next_pos+1:]\n",
    "                if next_word == word:\n",
    "                    post_list = post_list + \",\" + next_post_list\n",
    "                    next_new_line = input_files[next_ind].readline()[:-1]\n",
    "                    if next_new_line:\n",
    "                        heappush(heap,(next_new_line,next_ind))\n",
    "                    else:\n",
    "                        i+=1\n",
    "                else:\n",
    "                    heappush(heap,(next_s,next_ind))\n",
    "                    break\n",
    "\n",
    "            if word not in word_pos:\n",
    "                word_pos[word]=dict()\n",
    "\n",
    "            word_pos[word][x] = output_files[outfile_index].tell()\n",
    "            postings = post_list.split(\",\")\n",
    "            documents = dict()\n",
    "            idf = log10(page_count/len(postings))\n",
    "\n",
    "            for post in postings:\n",
    "                pos = post.find(\":\")\n",
    "                id = post[:pos]\n",
    "                freq = int(post[pos+1:])\n",
    "                tf = log10(freq)+1\n",
    "                documents[str(id)] = round(tf*idf,2)\n",
    "\n",
    "            documents = sorted(documents.items(),key=operator.itemgetter(1), reverse=True)\n",
    "            top_post_list_result = \"\"\n",
    "\n",
    "            for doc in documents:\n",
    "                top_post_list_result = top_post_list_result + doc[0] + \":\" + str(doc[1]) + \",\"\n",
    "            top_post_list_result = top_post_list_result[:-1]\n",
    "            output_files[outfile_index].write(top_post_list_result+\"\\n\")\n",
    "\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    output_files[outfile_index].close()\n",
    "\n",
    "    try:\n",
    "        for i in range(file_count):\n",
    "            file = index_path + \"/\" + x + str(i) + \".txt\"\n",
    "            input_files[i].close()\n",
    "            os.remove(file)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(index_path + \"/word_postions.pickle\",\"wb+\")\n",
    "pickle.dump(word_pos,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(\"Total time taken : \" + str(end-start) + \" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
